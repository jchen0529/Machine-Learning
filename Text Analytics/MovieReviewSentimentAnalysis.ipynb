{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Reviews Sentiment Analysis\n",
    "\n",
    "**Outline**\n",
    "\n",
    "* [Introduction and dataset](#data)\n",
    "* [Feature/Model variations](#model)\n",
    "   * [M1 - Unigrams (absence/presence)](#uni)\n",
    "   * [M2 - Unigrams with frequency count](#uni_multi)\n",
    "   * [M3 - Unigrams (only adjectives/adverbs)](#adjadv)\n",
    "   * [M4 - Unigrams (sublinear tf-idf), apply stopword removal](#tfidf)\n",
    "      * [tf-idf introduction](#tfidf_deets)\n",
    "   * [M5 - Bigrams (absence/presence)](#bigram)\n",
    "* [Model learnings](#summary)\n",
    "* [References](#ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import string, re\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer #leaving only the word stem\n",
    "from nltk import pos_tag\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"data\">Introduction and Dataset</a>\n",
    "\n",
    "The goal of this project is to classify moview review sentiments (positive or negative) using multinomial Naive Bayes, experiment with different language models and techniques along th way to evaluate performances and compare pros and cons of different methods.\n",
    "\n",
    "**Steps included:**\n",
    "* Data read in\n",
    "* Split into train/test before vectorization\n",
    "* Clean corpus based on language model specs and apply additional techniques such as stop word removal, lemmatization as needed for best performance\n",
    "* Train model and predict on test set\n",
    "* Calculate accuracy score and compare\n",
    "\n",
    "Here's a graph from [Text Analytics for NLTK Beginners](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk) that help illustrate the process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Text Classification Process Flow](img/ta_flow_chart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Readin**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a pos/neg dir\n",
    "# each document is a review\n",
    "corpus_folder = './data/review_polarity/txt_sentoken/'\n",
    "\n",
    "# Function to read one document\n",
    "def readFile(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        tokenzied_words = f.read().split()\n",
    "        return tokenzied_words\n",
    "\n",
    "def readDir(senti_folder, pattern, top_doc_num):\n",
    "    \"\"\"This funtion reads in data from the respective folder until filname starts with top_doc_num.\n",
    "    \n",
    "    Args:\n",
    "        senti_folder: filepath to the corpus\n",
    "        pattern: file name pattern\n",
    "        top_doc_num: number included in file name to stop data read in\n",
    "    Returns:\n",
    "        Returns a list of the corpus. \n",
    "    \"\"\"\n",
    "    file_list = []\n",
    "    path_pattern = os.path.join(corpus_folder, senti_folder, pattern + '*.txt')\n",
    "    all_txt_paths = glob.glob(path_pattern)\n",
    "    for file_path in all_txt_paths[:top_doc_num]:\n",
    "        # print(file_path)\n",
    "        word_List = readFile(file_path)\n",
    "        # print(word_List)\n",
    "        file_list.append(word_List)\n",
    "    return file_list\n",
    "\n",
    "#Read in train, test lists for pos and neg reviews, create train and test labels\n",
    "train_pos_file_list = readDir('pos', 'cv[0-8]', top_doc_num = 900)\n",
    "train_neg_file_list = readDir('neg', 'cv[0-8]', top_doc_num = 900)\n",
    "train_pos_labels = [1 for i in range(len(train_pos_file_list))]\n",
    "train_neg_labels = [0 for i in range(len(train_neg_file_list))]\n",
    "\n",
    "test_pos_file_list = readDir('pos', 'cv9', top_doc_num = 1000)\n",
    "test_neg_file_list = readDir('neg', 'cv9', top_doc_num = 1000)\n",
    "test_pos_labels = [1 for i in range(len(test_pos_file_list))]\n",
    "test_neg_labels = [0 for i in range(len(test_neg_file_list))]\n",
    "\n",
    "train_file_list = train_pos_file_list + train_neg_file_list\n",
    "test_file_list = test_pos_file_list + test_neg_file_list\n",
    "\n",
    "train_labels = train_pos_labels + train_neg_labels\n",
    "test_labels = test_pos_labels + test_neg_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"model\">Feature/Model variations</a>\n",
    "**M1 - Unigrams (absence/presence)/Bernoulli Naive Bayes**\n",
    "* training corpus: tokenized set of unique words that appeared in training corpus\n",
    "* technique: Stop words removal, Porter Stemmer\n",
    "\n",
    "**M2 - Unigrams with frequency count**\n",
    "* training corpus: tokenized entire vocabulary\n",
    "* technique: Porter Stemmer\n",
    "\n",
    "**M3 - Unigrams (only adjectives/adverbs)**\n",
    "* training corpus for training: set of tagged adjectives and adverbs only\n",
    "* technique: Part of Speech (POS) Tagging\n",
    "\n",
    "**M4 - Unigrams (sublinear tf-idf), apply stopword removal**\n",
    "* training corpus: tokenized entire vocabulary\n",
    "* technique: Porter Stemmer\n",
    "\n",
    "**M5 - Bigrams (absence/presence)**\n",
    "* training corpus: tokenized entire vocabulary\n",
    "* technique: Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_corpus(tokenized, model):\n",
    "    \"\"\"This funtion reads in data from each individual review in the training corpora and keeps words/features \n",
    "    based on model specification \n",
    "    \n",
    "    Args:\n",
    "        tokenized: individual review in training corpora (list of reviews)\n",
    "        model: model type\n",
    "    Returns:\n",
    "        Returns a list of the corpus under model spec. \n",
    "    \"\"\"\n",
    "    #tokenize each document in training corpora\n",
    "    punctuation_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x)]\n",
    "    \n",
    "    #Apply porter stemmer to selected models to only keep word stem\n",
    "    ps = PorterStemmer()\n",
    "    stemmed = [ps.stem(word) for word in punctuation_free]\n",
    "    \n",
    "    #unigrams with absence/presence\n",
    "    if model == \"m1\":\n",
    "        unique_stemmed = set(stemmed)\n",
    "        return ' '.join(unique_stemmed)\n",
    "    \n",
    "    #Unigrams with only adjectives/adverbs\n",
    "    elif model == \"m3\":\n",
    "    #else:\n",
    "        tags = ['JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS']\n",
    "        all_tags = pos_tag(punctuation_free)\n",
    "        result = [word[0] for word in all_tags if word[1] in tags]\n",
    "        result2 = ' '.join(result)\n",
    "        #print(\"output results for m3\")\n",
    "        return result2\n",
    "    \n",
    "    #Unigrams with frequency count\n",
    "    elif model == \"m2\" or \"m4\" or \"m5\":\n",
    "        #print(\"output results for m2/4\")\n",
    "        return ' '.join(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_train_predict(train_clean, test_clean):\n",
    "    #update list of stop words to include film, movie, etc that doesn't give signals to sentiment\n",
    "    my_stop_words = text.ENGLISH_STOP_WORDS.union([\"movie\", \"film\", \"movi\", \"hi\"])\n",
    "    \n",
    "    #Create features\n",
    "    vectorizer = CountVectorizer(lowercase=True,stop_words=my_stop_words) #count word frequency\n",
    "    train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "    test_features = vectorizer.transform([doc for doc in test_file_list_clean])\n",
    "\n",
    "    nb_clf = MultinomialNB()\n",
    "    \n",
    "    # Fit model and predict on test features\n",
    "    nb_clf.fit(train_features, train_labels)\n",
    "    predictions = nb_clf.predict(test_features)\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    return vectorizer, nb_clf, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_most_informative_features(vectorizer, classifier, n=5):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names = vectorizer.get_feature_names()  \n",
    "    topn_pos_class = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_neg_class = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]    \n",
    "\n",
    "    print(\"Important words in positive reviews\")\n",
    "    for coef, feature in topn_pos_class:\n",
    "        print(class_labels[1], coef, feature) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in negative reviews\")\n",
    "    for coef, feature in topn_neg_class:\n",
    "        print(class_labels[0], coef, feature)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"uni\">M1 - Unigrams (absence/presence)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans each individual review and keeps word features based on model specification \n",
    "train_file_list_clean = [clean_corpus(doc, \"m1\") for doc in train_file_list]\n",
    "test_file_list_clean = [clean_corpus(doc,\"m1\") for doc in test_file_list]\n",
    "\n",
    "m1_vector = model_train_predict(train_file_list_clean, test_file_list_clean)[0]\n",
    "m1 = model_train_predict(train_file_list_clean, test_file_list_clean)[1]\n",
    "m1_accuracy = model_train_predict(train_file_list_clean, test_file_list_clean)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1_accuracy is:  0.865\n",
      "Important words in positive reviews\n",
      "1 852.0 thi\n",
      "1 743.0 ha\n",
      "1 677.0 like\n",
      "1 668.0 make\n",
      "1 666.0 time\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 864.0 thi\n",
      "0 716.0 ha\n",
      "0 714.0 like\n",
      "0 690.0 wa\n",
      "0 648.0 make\n"
     ]
    }
   ],
   "source": [
    "print(\"m1_accuracy is: \",m1_accuracy,)\n",
    "show_most_informative_features(m1_vector, m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"uni_multi\">M2 - Unigrams with frequency count</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleans each individual review and keeps word features based on model specification \n",
    "train_file_list_clean = [clean_corpus(doc, \"m2\") for doc in train_file_list]\n",
    "test_file_list_clean = [clean_corpus(doc,\"m2\") for doc in test_file_list]\n",
    "\n",
    "m2_vector = model_train_predict(train_file_list_clean, test_file_list_clean)[0]\n",
    "m2 = model_train_predict(train_file_list_clean, test_file_list_clean)[1]\n",
    "m2_accuracy = model_train_predict(train_file_list_clean, test_file_list_clean)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825\n",
      "Important words in positive reviews\n",
      "1 4158.0 thi\n",
      "1 2329.0 ha\n",
      "1 2224.0 wa\n",
      "1 1758.0 like\n",
      "1 1752.0 charact\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 4420.0 thi\n",
      "0 2199.0 wa\n",
      "0 1974.0 ha\n",
      "0 1842.0 like\n",
      "0 1568.0 charact\n"
     ]
    }
   ],
   "source": [
    "print(m2_accuracy)\n",
    "\n",
    "show_most_informative_features(m2_vector, m2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"adjadv\">M3 - Unigrams (only adjectives/adverbs)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleans each individual review and keeps word features based on model specification \n",
    "train_file_list_clean = [clean_corpus(doc, \"m3\") for doc in train_file_list]\n",
    "test_file_list_clean = [clean_corpus(doc,\"m3\") for doc in test_file_list]\n",
    "\n",
    "m3_vector = model_train_predict(train_file_list_clean, test_file_list_clean)[0]\n",
    "m3 = model_train_predict(train_file_list_clean, test_file_list_clean)[1]\n",
    "m3_accuracy = model_train_predict(train_file_list_clean, test_file_list_clean)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n",
      "Important words in positive reviews\n",
      "1 1199.0 just\n",
      "1 1099.0 good\n",
      "1 729.0 best\n",
      "1 693.0 really\n",
      "1 691.0 little\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 1391.0 just\n",
      "0 1012.0 good\n",
      "0 928.0 bad\n",
      "0 709.0 really\n",
      "0 660.0 little\n"
     ]
    }
   ],
   "source": [
    "print(m3_accuracy)\n",
    "\n",
    "show_most_informative_features(m3_vector, m3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"tfidf\">M4 - Unigrams (sublinear tf-idf), apply stopword removal</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/siliangchen/anaconda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1089: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "train_file_list_clean = [clean_corpus(doc, \"m4\") for doc in train_file_list]\n",
    "test_file_list_clean = [clean_corpus(doc,\"m4\") for doc in test_file_list]\n",
    "\n",
    "#update list of stop words to include film, movie, etc that doesn't give signals to sentiment\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS.union([\"movie\", \"film\", \"movi\", \"hi\"])\n",
    "\n",
    "# Create features for TF-IDF\n",
    "# min_df: ignore terms that have a document frequency strictly lower than defined, default=1\n",
    "# max_df: ignore terms that have appear in more than 80% of the documents, default=1\n",
    "vectorizer = TfidfVectorizer(min_df = 2, max_df = 0.8, stop_words=my_stop_words, sublinear_tf=True) \n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])\n",
    "nb_clf = MultinomialNB()\n",
    "\n",
    "# Fit model and predict on test features\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.84\n",
      "Important words in positive reviews\n",
      "1 21.422062346052442 wa\n",
      "1 19.37887511027754 charact\n",
      "1 19.269493613295754 like\n",
      "1 18.533446429908274 make\n",
      "1 18.14754763977736 time\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 23.292004621773454 wa\n",
      "0 21.930512105992694 like\n",
      "0 19.969588038385954 charact\n",
      "0 19.688306525881615 just\n",
      "0 18.638530042001538 bad\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"tfidf_deets\">tf-idf introduction</a>\n",
    "\n",
    "**What is [Tf-idf](tfidf.come)?**\n",
    "\n",
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. \n",
    "\n",
    "Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "**TF: Term Frequency**\n",
    "TF measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear many more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization. \n",
    "\n",
    "$$TF(t) = (NumberOfTimes_Term_t_AppearsInDoc)/(DocWordCount)$$\n",
    "\n",
    "IDF: Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "$$IDF(t) = log_e(TotalNumberOfDocs / NumberOfDocs_Term_t_AppearsIn)$$\n",
    "\n",
    "**Example**\n",
    "\n",
    "Consider a document containing 100 words wherein the word **cat** appears 3 times: $tf=(3 / 100) = 0.03$\n",
    "\n",
    "we have 10 million documents and the word **cat** appears in 1000 of these: $idf=log(10,000,000 / 1,000) = 4$\n",
    "\n",
    "Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"bigram\">M5 - Bigrams (absence/presence)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cleans each individual review and keeps word features based on model specification \n",
    "train_file_list_clean = [clean_corpus(doc, \"m5\") for doc in train_file_list]\n",
    "test_file_list_clean = [clean_corpus(doc,\"m5\") for doc in test_file_list]\n",
    "\n",
    "#Create features\n",
    "vectorizer = CountVectorizer(lowercase=True,ngram_range = (2,2), binary = True) #count word frequency\n",
    "train_features = vectorizer.fit_transform([doc for doc in train_file_list_clean])\n",
    "test_features = vectorizer.transform([doc for doc in test_file_list_clean])\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "\n",
    "# Fit model and predict on test features\n",
    "nb_clf.fit(train_features, train_labels)\n",
    "predictions = nb_clf.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.855\n",
      "Important words in positive reviews\n",
      "1 845.0 of the\n",
      "1 787.0 in the\n",
      "1 684.0 the film\n",
      "1 657.0 to the\n",
      "1 635.0 and the\n",
      "-----------------------------------------\n",
      "Important words in negative reviews\n",
      "0 828.0 of the\n",
      "0 796.0 in the\n",
      "0 643.0 the film\n",
      "0 625.0 to be\n",
      "0 577.0 to the\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(test_labels, predictions)\n",
    "print(accuracy)\n",
    "\n",
    "show_most_informative_features(vectorizer, nb_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"summary\">Model learnings</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, based on accuracy, the best performing model is **M1 Unigram absense/presence model with Porter Stemmer applied** with 86.5% in accuracy. The model is essentially a Bernoulli Naive Bayes model representing the feture vectors in a document with binary elements, in other words, whether a word in the vocabulary is present or not (1 or 0). \n",
    "\n",
    "Followed by **M5 Bigram with Porter Stemmer applied** with 85.5% in accuracy. \n",
    "Followed by **M3 Unigram with adj/adv** with 85% in accuracy. \n",
    "\n",
    "The reason these these models performed the best could be that M1 only considers the absence or presence of each word therefore it does not weight more to words that appear the most frequent (M2, which was performed the worst). M2 uses bigram to create feature vectors, the phrases consisted of two words could give more signal to sentiments. M3 only considers the count of adjectives and adverbs that often more indicative of the sentiment of a review and therefore helps with precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ref\">References</a>\n",
    "* [Tf-idf](http://www.tfidf.com/)\n",
    "* [Text Analytics for NLTK Beginners](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
