{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification using Naive Bayes Classifier\n",
    "\n",
    "**Outline**\n",
    "\n",
    "* [Introduction and what is Naive Bayes](#intro)\n",
    "* [Simple example](#exp)\n",
    "* [Bayes theorem](#bayes)\n",
    "* [Being naive](#naive)\n",
    "* [Apply smoothing for unknown words](#smooth)\n",
    "* [Implementing Multinomial Naive Bayes using sklearn](#mnb)\n",
    "* [Naive Bayes pros and cons](#pc)\n",
    "* [Additional techniques to improve model](#add)\n",
    "* [References](#ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"intro\">Introduction</a>\n",
    "\n",
    "My learning of text classification began with training a naive bayes classifier to predict the category of a text. This notebook will summarize the basic algorithm and how to implement a simple naive bayes model. \n",
    "\n",
    "** What is Naive Bayes?**\n",
    "\n",
    "A linear classifier/probabilistic model based on Bayesâ€™ theorem, *naive* comes from the assumption that the features in a dataset are mutually independent which is often violated in practice. Naive Bayes classifiers still tend to perform well under this unrealistic assumption, especially for small sample sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"exp\">Simple example</a>\n",
    "\n",
    "Here I followed the example in this blog [A practical explanation of a Naive Bayes classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/#advanced-techniques) to show how Naive Bayes can be applied to learn the tag of texts. See training data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>vocab size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sports</td>\n",
       "      <td>A great game</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Sports</td>\n",
       "      <td>The election was over</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sports</td>\n",
       "      <td>Very clean match</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sports</td>\n",
       "      <td>A clean but forgettable game</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Sports</td>\n",
       "      <td>It was a close election</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                          text  vocab size\n",
       "0      Sports                  A great game           3\n",
       "1  Not Sports         The election was over           4\n",
       "2      Sports              Very clean match           3\n",
       "3      Sports  A clean but forgettable game           5\n",
       "4  Not Sports       It was a close election           5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'text': ['A great game',\n",
    "                            'The election was over',\n",
    "                            'Very clean match',\n",
    "                            'A clean but forgettable game',\n",
    "                            'It was a close election'],\n",
    "                   'category':['Sports', 'Not Sports', 'Sports', 'Sports', 'Not Sports']})\n",
    "\n",
    "df['vocab size'] = df['text'].str.split().apply(len)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give this training data, now let's train a classifier to predict the category for a given sentence 'a very close game'. Since Naive Bayes is a probabilistic model, we want to know P(Sports|a very close game) and P(Not Sports|a very close game) and check which one is bigger/more likely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"bayes\">Bayes theorem</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(A | B) = \\frac{P(B | A) P(A)}{P(B)}$$\n",
    "\n",
    "In our case, we want to know: $$P(Sports|a very close game) = \\frac{P(a very close game | sports) P(sports)}{P(a very close game)}$$\n",
    "\n",
    "and: $$P(Not Sports|a very close game) = \\frac{P(a very close game | Not sports) P(Not sports)}{P(a very close game)}$$\n",
    "\n",
    "Since we are interested in which one returns a larger probability, we can focus on calculating the nominators and compare the values.\n",
    "\n",
    "We can first calculate the a priori probability of each tag: For a given sentence in the training corpus, $P(sports) = 3/5$ and $P(not sports) = 2/5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"naive\">Being Naive</a>\n",
    "We assume each word in a sentence is **independent** of the other ones, and thus focusing on each word of the sentence. Under this assumption, \"the party was fun\" is the same with \"fun party was the\".\n",
    "\n",
    "We can rewrite $P(a very close game)$ As $P(a)*P(very)*P(close)*P(game)$\n",
    "thus \n",
    "$$P(a very close game|sports)=P(a|sports)*P(very|sports)*P(close|sports)*P(game|sports)$$\n",
    "\n",
    "Now we can go ahead and calculate these probabilities, which is just counting the frequency of words in our training corpus.\n",
    "\n",
    "The problem arise for P(close|sports) because the word 'close' does not appear in our training corpus, if we regard it as 0 then the calculation is nullified and won't give us any information on $P(a very close game|sports)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"smooth\">Apply smoothing for unknown words</a>\n",
    "\n",
    "Smoothing is used when a word doesn't appear in training but appears in testing. \n",
    "\n",
    "**Add-one/La Place smoothing** is an additive smoothing method that adds one to each word that appeared in the test document, which is essentially pretending that we saw each word one more time than we did; and add the vocabulary size of the entire training document (V) to the denominator, so that the division will never be greater than 1.\n",
    "\n",
    "$$ P_{MLE}(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i)}{c(w_{i-1})}$$\n",
    "$$ P_{Add-1}(w_i | w_{i-1}) = \\frac{c(w_{i-1}, w_i) + 1}{c(w_{i-1}) + V}$$\n",
    "\n",
    "**Issues with Add-one smooth**: When the number of zeros is huge, the total probability of novel events are large. Improvements include Good-Turing smoothing, and lambda smoothing, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "Not Sports     9\n",
       "Sports        11\n",
       "Name: vocab size, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vocab size of each category\n",
    "size = df.groupby('category')[\"vocab size\"].sum()\n",
    "size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total vocab size of the training corpus v = 14.\n",
    "Therefore, $$P(close|Sports) = (0+1)/(11 + 14) = 1/25$$\n",
    "$$P(close|Not Sports) = (0+1)/(9 + 14) = 2/23$$\n",
    "\n",
    "Final results of $P(Sports|a very close game) = 0.0000276$ which is larger than $P(Not Sports|a very close game) = 0.00000572$, and our classifier gives \"a very close game\" the **sports** tag!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"mlb\">Implementing Multinomial Naive Bayes using Sklearn</a>\n",
    "\n",
    "Default smoothing parameter alpha is 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiNB_fit(df, x_colname, y_colname):\n",
    "    \"\"\"\n",
    "    fit multinomial naive bayes model.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): a dataframe having the document and label\n",
    "        x_colname (str): the colname for the document column\n",
    "        y_colname (str): the colname for the label column       \n",
    "    \n",
    "    Returns:\n",
    "       nb_clf: A Sklearn MultinomialNB object\n",
    "       vect: the feature vectors obtained from training data\n",
    "    \"\"\"\n",
    "    \n",
    "    # get document and label\n",
    "    X_train = df['text']\n",
    "    y_train = df['category']\n",
    "    \n",
    "    # vectorize the document for both train and test\n",
    "    vect = CountVectorizer()\n",
    "    X_train_feats = vect.fit_transform(X_train)\n",
    "\n",
    "    # See the result of the vectorization\n",
    "    print('feature name: ', vect.get_feature_names())\n",
    "\n",
    "    # convert to dense array for better visualize representation\n",
    "    print('training:')\n",
    "    print(X_train_feats.toarray())\n",
    "\n",
    "    # Fit Multinomial NB model and predict the final probability\n",
    "    nb_clf = MultinomialNB()\n",
    "    nb_clf.fit(X_train_feats, y_train)\n",
    "            \n",
    "    return nb_clf, vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature name:  ['but', 'clean', 'close', 'election', 'forgettable', 'game', 'great', 'it', 'match', 'over', 'the', 'very', 'was']\n",
      "training:\n",
      "[[0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 1 1 0 1]\n",
      " [0 1 0 0 0 0 0 0 1 0 0 1 0]\n",
      " [1 1 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 1 0 0 0 1 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# fit Multinomial Naive Bayes Model\n",
    "nb_clf, vect = multiNB_fit(df, x_colname='text', y_colname='category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multiNB_predict(nb_clf, vect, x_test, predict_class=True):\n",
    "    \"\"\"\n",
    "    predict the classification result using the trained nb_clf.\n",
    "    \n",
    "    Args:\n",
    "        nb_clf (sklearn.naive_bayes.MultinomialNB): Sklearn MultinomialNB object\n",
    "        vect (CountVectorizer): the feature vectors obtained from training data\n",
    "        x_test (pd.Series): a pd.Series contains the document for testing\n",
    "        predict_class (bol): whether to return the predicted class or probability\n",
    "    Returns:\n",
    "        array contains predicted class or probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    # vectorize the test document \n",
    "    X_test_feats = vect.transform(x_test)\n",
    "    \n",
    "    # convert to dense array for better visualize representation\n",
    "    print('\\ntesting:')\n",
    "    print(X_test_feats.toarray()) \n",
    "    \n",
    "    ### predict result\n",
    "    if (predict_class==True):        \n",
    "        pred = nb.predict(X_test_feats)\n",
    "    else:\n",
    "        pred = nb.predict_proba(X_test_feats)\n",
    "                    \n",
    "    print('Predicted results:', pred)\n",
    "    \n",
    "    return pred  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A close game</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It was a forgettable election</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            text\n",
       "0                   A close game\n",
       "1  It was a forgettable election"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# manually input test data\n",
    "x_test = pd.DataFrame({'text': ['A close game',\n",
    "                               'It was a forgettable election']})\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:\n",
      "[[0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 1 0 0 0 0 1]]\n",
      "Predicted results: ['Sports' 'Not Sports']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Sports', 'Not Sports'], dtype='<U10')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the final classification label for testing data\n",
    "multiNB_predict(nb, vect, x_test['text'], predict_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "testing:\n",
      "[[0 0 1 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 1 0 0 0 0 1]]\n",
      "Predicted results: [[0.32785775 0.67214225]\n",
      " [0.87845067 0.12154933]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.32785775, 0.67214225],\n",
       "       [0.87845067, 0.12154933]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the final classification label for testing data\n",
    "multiNB_predict(nb, vect, x_test['text'], predict_class=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"pc\">Naive Bayes pros and cons</a>\n",
    "\n",
    "**pros:**\n",
    "* No parameter tuning is required\n",
    "* Simple and easy to implement and isn't computationally expensive.\n",
    "* Highly scalable. It scales linearly with the number of predictors and data points.\n",
    "* Can be used for both binary and multiclass classification problems.\n",
    "* Not sensitive to irrelevant features.\n",
    "\n",
    "**cons:**\n",
    "* strong assumption on feature independence\n",
    "* data scarcity which would require smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"add\">Additional techniques to improve model</a>\n",
    "\n",
    "**Remove stop words:** remove common words that don't add anything meaningful to the classification such as: the, a, was, it.\n",
    "\n",
    "**Lemmatize words:** group together different inflections of the same word. EX: election, elections, elected, would be grouped together to elect and counted together.\n",
    "\n",
    "**Use n-grams:** tokenize to phrases of more than one word and count these phrases instead of single words\n",
    "\n",
    "**Use TF-IDF:** Instead of just counting frequency, we could do something more advanced like penalizing words that appear frequently in most of the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ref\">References</a>\n",
    "* [Text Classification using Naive Bayes from Johnny Chiu](https://nbviewer.jupyter.org/github/johnnychiuchiu/Machine-Learning/blob/master/TextAnalytics/naiveBayesTextClassification.ipynb#laplace)\n",
    "* [A practical explanation of a Naive Bayes classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/#advanced-techniques)\n",
    "* [scikit learn: Naive Bayes](https://scikit-learn.org/stable/modules/naive_bayes.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
