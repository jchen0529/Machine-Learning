{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling through Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "**Outline**\n",
    "\n",
    "* [Introduction](#intro)\n",
    "* [Example](#exp)\n",
    "* [Airbnb use case](#use case)\n",
    "* [References](#ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "# construct the dictionary without loading all texts into memory\n",
    "from six import iteritems\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import pickle\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"data\">Introduction and Dataset</a>\n",
    "\n",
    "**What is LDA model?**\n",
    "LDA represents documents as **mixture of topics** that spits out words with certain probabilities. It is a bag-of-words model.\n",
    "\n",
    "**What does LDA do?**\n",
    "It is a way of automatically discovering **topics** that sentences contain and is unsuperivised learning.\n",
    "\n",
    "**LDA model assumption of how documents are written**\n",
    "It assumes that when writing each document, you:\n",
    "\n",
    "* decide on the number of words N the document will have (say, to a Poisson distribution)\n",
    "* choose a topic mixture for the document ex: have a document to be about 1/3 food and 2/3 animals\n",
    "* generate each word w_i in the document by:\n",
    "    + first picking a topic (based on the topic's multinomial distribution)\n",
    "    + use the topic to generate the word itself\n",
    "\n",
    "Assuming this generative model for a collection of documents, LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the collection.\n",
    "\n",
    "**How does LDA learn?**\n",
    "\n",
    "Below is an example where we have 5 documents, and we wanted to choose 2 topics to discover, then use LDA to learn the topic representation of each document and the words associated to each topic. How do you do this?\n",
    "\n",
    "One method is using collapsed Gibbs sampling:\n",
    "\n",
    "* go through each document, and randomly assign each word in the document to one of the K topics.\n",
    "    * This random assignment already gives you both topic representations to all the documents and word distributions of all the topics (although not very good)\n",
    "* to improve on the random guessing, for each document d_i:\n",
    "    * go through each word w_i in d_i:\n",
    "        * for each topic t, compute two things:\n",
    "            * p(topic t | document d_i) = prop of words in d_i that are currently assigned to topic t\n",
    "            * p(word w_i | topic t) = prop of assignments to topic t over all documents that come from this word w_i\n",
    "            * Reassign with a new topic, where we choose topic t with probability p(topic t | document d_i) * p(word w_i | topic t)\n",
    "            * In other words, we are assuming that all topic assignments except for the current word in question are correct, then update the assignment of the current word using our model of how docs are generated.\n",
    "            \n",
    "* Repeat the previous step a large number of times and you will eventually reach a steady state where your assignments are pretty good. Then these assignments will be used to estimate the topic mixtures of each document.\n",
    "\n",
    "Reference: [Edward Chen's blog on intro to LDA](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LDA model representation](img/LDA_viz.png)\n",
    "\n",
    "Source: [Topic Model by David M. Blei](http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"exp\">Example</a>\n",
    "\n",
    "Here is a quick example to show how LDA works to discover topics in given documents.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Compile sample documents into a list\n",
    "2. Clean corpus => apply stop words, lemmatize verbs and nouns, return clean corpus\n",
    "3. Store clean corpus in dictionary and assign unique id for each unique term\n",
    "    {term: id}\n",
    "4. Apply doc2bow to convert tokenized documents into a document-term matrix, filter out non-frequent words to reduce size of matrix \n",
    "    + Each document is now a bag of words(bow)\n",
    "    + Since corpora size is already small, we will not filter out non-frequent words to demonstrate model*\n",
    "5. Run LDA model\n",
    "6. Tune parameters\n",
    "7. Evaluate model on new texts and save model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 1 & 2. Compile sample docs and clean corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample documents\n",
    "doc_1 = \"Don't feed monkeys fish. Monkeys love bananas. Cats love fish.\"\n",
    "doc_2 = \"Northwestern University is one of the most beautiful universities.\"\n",
    "doc_3 = \"2019 is the year of pig.\"\n",
    "doc_4 = \"Animals are our friends.\"\n",
    "doc_5 = \"There are many master programs at Northwestern University. MSIA is a strong program at Northwestern University.\"\n",
    "\n",
    "# compile sample documents into a list\n",
    "doc_list = [doc_1, doc_2, doc_3, doc_4, doc_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create English stop words list (you can always define your own stopwords)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop_words.add('.')\n",
    "#print(stop_words)\n",
    "\n",
    "# Create a WordNetLemmatizer object for lemmatization as needed\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stop words from sentences & lemmatize verbs and nouns. \n",
    "def clean(doc):\n",
    "    tokenized = word_tokenize(doc.lower())\n",
    "    stop_free = [x for x in tokenized if not re.fullmatch('[' + string.punctuation + ']+', x) and x not in stop_words]\n",
    "    lemma_verb = [lemmatizer.lemmatize(word,'v') for word in stop_free]\n",
    "    lemma_noun = [lemmatizer.lemmatize(word,'n') for word in lemma_verb]\n",
    "    y = [s for s in lemma_noun if len(s) > 2]\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"n't\",\n",
       "  'fee',\n",
       "  'monkey',\n",
       "  'fish',\n",
       "  'monkey',\n",
       "  'love',\n",
       "  'banana',\n",
       "  'cat',\n",
       "  'love',\n",
       "  'fish'],\n",
       " ['northwestern', 'university', 'one', 'beautiful', 'university'],\n",
       " ['2019', 'year', 'pig'],\n",
       " ['animal', 'friend'],\n",
       " ['many',\n",
       "  'master',\n",
       "  'program',\n",
       "  'northwestern',\n",
       "  'university',\n",
       "  'msia',\n",
       "  'strong',\n",
       "  'program',\n",
       "  'northwestern',\n",
       "  'university']]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_clean = [clean(doc.strip()) for doc in doc_list]\n",
    "corpus_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps 3 & 4. Store clean corpus and apply doc2bow **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'banana': 0, 'cat': 1, 'fee': 2, 'fish': 3, 'love': 4, 'monkey': 5, \"n't\": 6, 'beautiful': 7, 'northwestern': 8, 'one': 9, 'university': 10, '2019': 11, 'pig': 12, 'year': 13, 'animal': 14, 'friend': 15, 'many': 16, 'master': 17, 'msia': 18, 'program': 19, 'strong': 20}\n"
     ]
    }
   ],
   "source": [
    "# find a unique id for each unique term {term : id}\n",
    "dictionary = corpora.Dictionary(corpus_clean)\n",
    "# term : id\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 2, 5, 3, 4, 0, 1, 9, 7, 11, 13, 12, 14, 15, 16, 17, 19, 18, 20]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optional: Filter out non-frequent words\n",
    "# low_freq_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq ==1]\n",
    "# low_freq_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(2 unique tokens: ['northwestern', 'university'])\n"
     ]
    }
   ],
   "source": [
    "# dictionary.filter_tokens(low_freq_ids)\n",
    "# dictionary.compactify() # remove gaps in id sequence after words that were removed\n",
    "# print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 2), (4, 2), (5, 2), (6, 1)],\n",
       " [(7, 1), (8, 1), (9, 1), (10, 2)],\n",
       " [(11, 1), (12, 1), (13, 1)],\n",
       " [(14, 1), (15, 1)],\n",
       " [(8, 2), (10, 2), (16, 1), (17, 1), (18, 1), (19, 2), (20, 1)]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "# you will need to filter out non-frequent words\n",
    "corpus = [dictionary.doc2bow(doc_clean) for doc_clean in corpus_clean]\n",
    "# token, token freq\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5. Run LDA model **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA model needs many iterations/passes and a large corpus to work well\n",
    "# must define the number of topics you want to extract from the corpus\n",
    "ldamodel = LdaModel(corpus, num_topics=2, id2word = dictionary, passes=20, iterations=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6. Model parameters **\n",
    "\n",
    "* passes: Number of passes through the corpus during training. \n",
    "The *passes parameter* is indeed unique to gensim. It essentially allows LDA to see your corpus multiple times and is very handy for smaller corpora. LDA splits the corpus into mini-batches (for easier convergence - it is easier to work with smaller subsets) - so passes refers to how many times each mini-batch will be given to LDA.\n",
    "\n",
    "* iterations: Maximum number of iterations through the corpus when inferring the topic distribution of a corpus. The iterations parameter puts a limit on how many times LDA will execute the E-Step for each document meaning that some documents may not converge in time. One can set this as high as they like (or have time for).\n",
    "\n",
    "*hyperparameters such as alpha and eta, which tune the prior distributions for the document-topic multimual distribution and the topic-word multimual distribution.*\n",
    "\n",
    "* alpha: document-topic density, higher the value of alpha, documents are composed of more topics.\n",
    "\n",
    "* eta: A-priori belief on word probability/topic-word density, higher the eta, topics are composed of a large number of words in the corpus\n",
    "\n",
    "* decay (float, optional) â€“ A number between (0.5, 1] to weight what percentage of the previous lambda value is forgotten when each new document is examined.\n",
    "\n",
    "Reference: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7. Evaluate model and save**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.163*\"university\" + 0.127*\"northwestern\" + 0.091*\"program\" + 0.055*\"master\" + 0.055*\"msia\" + 0.055*\"strong\"'), (1, '0.106*\"love\" + 0.106*\"monkey\" + 0.106*\"fish\" + 0.063*\"fee\" + 0.063*\"n\\'t\" + 0.063*\"banana\"')]\n"
     ]
    }
   ],
   "source": [
    "# print out the top words in each topic\n",
    "print(ldamodel.print_topics(num_topics=2, num_words=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5), (1, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "new_texts = [\n",
    "    ['chicken', 'duck', 'farm', 'rice'],\n",
    "    ['northwestern', 'college', 'MSIA']\n",
    "]\n",
    "other_corpus = [dictionary.doc2bow(text) for text in new_texts]\n",
    "\n",
    "unseen_doc1 = other_corpus[0]\n",
    "\n",
    "# get topic probability distribution for an unseen_doc\n",
    "vector = ldamodel[unseen_doc1]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7458668), (1, 0.25413322)]\n"
     ]
    }
   ],
   "source": [
    "unseen_doc2 = other_corpus[1]\n",
    "\n",
    "# get topic probability distribution for an unseen_doc\n",
    "vector = ldamodel[unseen_doc2]\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The model didn't learn from enough texts to correctly classify the animals vector, but did correctly classify the school vector to topic 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dump model\n",
    "lda_model_file = open('lda_model.pkl','wb')\n",
    "pickle.dump(ldamodel,lda_model_file)\n",
    "lda_model_file.close()\n",
    "\n",
    "# load\n",
    "lda_model_file = open('lda_modelsilch.pkl','rb')\n",
    "ldamodel = pickle.loads(lda_model_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"use case\">Airbnb use case</a>\n",
    "\n",
    "Source: [Discovering and Classifying In-app Message Intent at Airbnb](https://medium.com/airbnb-engineering/discovering-and-classifying-in-app-message-intent-at-airbnb-6a55f5400a0c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background**\n",
    "There are emergency scenarios during booking can cause anxiety and confusion and answering questions in a real-time fasion is especially desirable. Airbnb developed a ML framework to mitigate the problem and it was applied on the in-app messaging platform. \n",
    "\n",
    "**Solution**\n",
    "\n",
    "**Identify message intent**automatically classify message intent to help shorten the response time for guests and reduce overall workload required for hosts.\n",
    "\n",
    "Phase 1: LDA to discover potential topics (intents) in the large message corpus\n",
    "Phase 2: moved to supervised learning techniques, use the topics derived from Phase 1 as intent labels for each message. *A multi-class classificaiton model using CNN was built \n",
    "   * There are messages with multi-intent, Airbnb treated sentences assigned with one single intent as an independent training sample when building the intent classification model.\n",
    "   * CNN was used due to implementation simplicity, reported high accuracy and fast speed at both training and inference time. \n",
    "\n",
    "The two phases worked together to accurately understand text data on Airbnb's messaging platform.\n",
    "\n",
    "** Model Productionization Flow Chart**\n",
    "![Airbnb offline training & online serving workflow of Phase 2](img/airbnb_model_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"ref\">References</a>\n",
    "* [Edward Chen's blog on intro to LDA](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)\n",
    "* [Topic Model by David M. Blei](http://www.cs.columbia.edu/~blei/papers/BleiLafferty2009.pdf)\n",
    "* [Discovering and Classifying In-app Message Intent at Airbnb](https://medium.com/airbnb-engineering/discovering-and-classifying-in-app-message-intent-at-airbnb-6a55f5400a0c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
